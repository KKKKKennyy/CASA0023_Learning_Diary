[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LearningDiary",
    "section": "",
    "text": "Preface\nThis is a Learning Diary of CASA0023 Remote Sensing."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "LearningDiary",
    "section": "About me",
    "text": "About me\n\n\n\nyy’s portrait\n\n\nKKKKKenny,"
  },
  {
    "objectID": "index.html#about-layout-of-learning-diary",
    "href": "index.html#about-layout-of-learning-diary",
    "title": "LearningDiary",
    "section": "About Layout of Learning Diary",
    "text": "About Layout of Learning Diary\nYou can also click bar in the left to navigate to corresponding page\n\nWeek 01\nWeek 02\nWeek 03"
  },
  {
    "objectID": "Week01.html",
    "href": "Week01.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Week02.html",
    "href": "Week02.html",
    "title": "Week 02 Portfolio",
    "section": "",
    "text": "If that preview doesn’t work, please click Link to find out more."
  },
  {
    "objectID": "Week03.html",
    "href": "Week03.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Week01.html#theoretical-part",
    "href": "Week01.html#theoretical-part",
    "title": "Week01 Introduction to remote sensing",
    "section": "1. Theoretical Part",
    "text": "1. Theoretical Part\n\n\n\nMindmap for W1 lecture part\n\n\n\n1.1. Different Sensors\n\n\n\n\n\n\n\n\n\n\n\nEnergy\nReceive\nE.g.,\nApplications\n\n\n\n\nPassive\n❌\nReflected energy from sun (electromagnetic)\nHuman eye, Satellite sensor\nRemote Sensing, meteorological observation, astronomy\n\n\nActive\n✅\nElectromagnetic that actively emit\nRadar, X-ray, LiDAR\nranging, velocity measurement, terrain mapping, and target identification\n\n\n\n Source: NASA\nPassive sensors are more widely used in remote sensing of the Earth and environmental monitoring because they can provide multi-band information on a global scale.\n\nThese sensors do not need to actively send signals to the target, but rely on natural radiation, thus enabling energy-efficient, wide-area observations. For this reason, passive sensors are often mounted on satellites to enable remote sensing observations on a global scale.\nIn addition, passive sensors typically cover multiple wavelength bands, including visible, infrared, and ultraviolet. This broad-spectrum coverage allows them to capture a wide range of features and information about the Earth’s surface, from vegetation cover to surface temperature, in different wavelength bands.\n\n\n\n1.2. Different Orbits\n\n\n\n\n\n\n\n\n\n\nDefinition\nE.g.,\nApplications\n\n\n\n\nGeosynchronous Orbit (GSO)\nSatellite’s period matches the Earth’s rotation period, making the satellite’s position relatively fixed above the Earth\nThe GOES series (Geostationary Operational Environmental Satellites) by the United States and the Meteosat series by Europe\nWeather Forecasting, Climate Monitoring, and communications\n\n\nGeostationary Orbit (GEO)\nSatellite’s speed matches the Earth’s rotation, allowing it to remain stationary relative to a point on the Earth’s surface.\nThe International Telecommunications Satellite Organization, SES (Société Européenne des Satellites)\nCommunication, Observing Earth’s weather, environment, and climate changes, providing valuable remote sensing data\n\n\n\n\n\n1.3. Different Interactions\n\n1.3.1. Earth’s Surface\n\nEnergy will be obsorbed by the surface\nEnergy being transmitted through the surface\n\n Source:Ghimire, 2021\n\nBidirectional Reflectance Distribution Function (BRDF):\nDefinition: A two-dimensional function used to represent the intensity distribution of light entering a material or surface and then exiting the light at the other end.\nApplication: Simulates real-life light conditions, detects particle concentrations in the atmosphere, and detects air humidity in the soil.\nReactions that occur when interacting with the Earth’s surface：\nPolarization: When electromagnetic waves are affected by interference, scattering or reflection, the direction of vibration changes. It is often used to distinguish between different material properties because different types of materials have different polarization responses.\nFluorescence: A substance is excited to emit light that has a longer wavelength. It can usually be used to detect certain specific compounds or biological processes, and is therefore used in fields such as environmental pollution detection and soil characterisation.\n\n\n\n1.3.2. Atmosphere\nAtmospheric scattering\n Source:Bovensmann et al., 2011\nQ: Why is the sky blue (provided that air particles)\nA: Because when the sun is overhead, the shorter wavelength of blue light (as shown in the figure below) is more easily scattered, and thus the sky is observed to be blue. Whereas at sunset, the blue light is scattered away by the atmosphere and the orange-red light passes through the atmosphere into the eye, hence the sky is orange-red.\n Source: The University of Waikato Te Whare Wānanga o Waikato\nSimilarly, seawater has the ability to absorb and scatter. What we usually observe is blue light scattered by seawater.\nIn addition, black colour indicates that all light is absorbed and there is no reflection, scattering or projection. Therefore, the sea floor is black without light.\nWhite, on the other hand, is the result of all colours being absorbed in equal amounts and is the result of mixing various colours together.\n\n\n\n1.4. Data Format\nMajority of remotely sensed data is Raster which is built from pixel. I think this link analysing the difference between a Raster and a Vector is still quite well written and explains what a Raster is quite clearly. In addition, this link also explain raster very well. They also have other information like “Raster bands”, “Image and raster data organization” and so on.\n\nSeveral File Types: BIL, BSQ, BIP, GeoTIFF(Most seen). This Link provides a detailed introduction to those kind of files with examples.\nResolution:\n\n\nRadiometric: The amount of information in each pixel.\n\n\n8-bit can hold 0-255 possibilities, 11-bit can hold 0-2047 possibilities. Here it is perhaps more superficially understood that subtle colour differences can or cannot be represented.\nIn addition, different numbers of spectral channels correspond to different spectral ranges measured on different bands. Multiple spectral channels allow the capture of information in different bands of the surface, such as visible light, infrared, etc. For example, AVIRIS has 224 spectral channels.\n\n\nSpatial: The size of the actual area represented by each pixel point.\n\n\nThe higher the resolution, the finer the surface features captured.\n\n Source: NASA\n\nSpectral：Ability to distinguish spectral detail over different bands.\n\n\nHigher spectral resolution indicates that the data has more information in more bands and is able to distinguish between more landmark features and material composition.\nIn data, each band is stored in a dedicated grating layer. Spectral data can be discrete or continuous.\n\n Source: NIREOS\n\nTemporal: Time interval for acquiring data.\n\n\nThe following figure shows the distribution of time intervals versus pixel resolution for different satellites.\n\n Source: ESRI"
  },
  {
    "objectID": "Week01.html#practical-part",
    "href": "Week01.html#practical-part",
    "title": "Week01 Introduction to remote sensing",
    "section": "Practical Part",
    "text": "Practical Part\n(Waiting for updating)"
  },
  {
    "objectID": "Week01.html#what-i-learned-from-mistakes.",
    "href": "Week01.html#what-i-learned-from-mistakes.",
    "title": "Week01 Introduction to remote sensing",
    "section": "What I learned from mistakes.",
    "text": "What I learned from mistakes.\n(Waiting for updating)"
  },
  {
    "objectID": "Week01.html#reflection",
    "href": "Week01.html#reflection",
    "title": "Week 01 Introduction to remote sensing",
    "section": "3. Reflection",
    "text": "3. Reflection\nThe first week was a relatively basic introductory class, so overall it wasn’t too difficult to get a basic knowledge of an unfamiliar field.\nAlthough I had forgotten a bit about the physics when it came to wavelengths and frequencies, it was good to see that only some concepts were covered. The subsequent understanding and working principle are not affected.\nIn addition, the storage and processing of remote sensing data (raster) in class was also very confusing. But the good thing is that the Drop-in session immediately following was able to talk about and solve this problem. Each pixel point has a unique spectral map (if it’s a multi-band image), usually synthesised from Red, Green, and Blue. But it is also possible to choose different wavelengths for the synthesis, depending on the needs of the study.\nI’m thankful to ArcGIS, ESRI, and NASA for really providing a lot of valid information in their offical documents to help me make sense of it. For example, this link is a detailed description of the Raster data. Also thanks to ChatGPT for being able to give me some basic Q&A’s."
  },
  {
    "objectID": "Week02.html#xaringan",
    "href": "Week02.html#xaringan",
    "title": "Week 02 Portfolio",
    "section": "1. Xaringan",
    "text": "1. Xaringan\n\n1.1. Xaringan Sample\nXaringan is a new tool for me, it’s a package in R. It can knit rmd into a html for presentation, at first I thought it’s a hard thing to do, but actually with the help of css it can have a lot of functions and styles.And it is very clear to use ‘—’ to separate different pages.\nXaringan Sample\n\n\n1.2. Bugs I’ve encountered in using Xaringan for Slides (FYI)\n\nBoth Xaringan and XaringanExtra require remote installation, and the instructions need to be executed on Console instead of Terminal\nWhen the template file is created, the DT package from CARN should also be installed\nUnlike yml documents, ‘—’ does not need to appear in pairs in Xaringan\n\n\noption(htmltools.dir.version = FALSE)\n\n\nAvoid the unconcise directory structure that comes with versioning\nIn the header of the Rmd file, several CSS files, which will be invoked in sequence in Knitting, can be declared at the same time\nIf you want the entire slides font to be adjusted, you should declare this in your CSS file. The font declared in the header can only affect the font of the current page\nSetting eval = FALSE at the beginning of a code block allows the code not to run, but to be displayed on the web page. Setting echo = FALSE can display only the result of running the code without showing the code\n\n\n\n1.3. Useful Materials\n\nXaringan Template in RStudio\nFonts, Colors, Theme Settings"
  },
  {
    "objectID": "Week02.html#quarto-mark-down",
    "href": "Week02.html#quarto-mark-down",
    "title": "Week 02 Portfolio",
    "section": "2. Quarto Mark Down",
    "text": "2. Quarto Mark Down\nQuarto Markdown is an upgraded version of R markdown. qmd document can be rendered into html or pdf or other formats.\nCompared to HTML, qmd is much simpler and doesn’t require pairs of ‘&lt;&gt;’ to declare styles, just a few symbols (like *, _, #) to create some styles.\nIn addition, images can be inserted by providing a file path or link, and by default they fill the screen without worrying about placement.\n\n2.1. Embedded Xaringan in QMD\nIf that preview doesn’t work, please click Link to find out more.\n\n\n\n\n\n\n\n\n\n\n2.2. Bugs I’ve encountered in using QMD (FYI)\n\nYou should first declare “output-dir: docs” in the yml file, which is consistent with the default setting in GitHub pages deployment. Remember not to type docs as docx, or you will get an error in the GitHub Action section.\nAll changes in Rstudio must render before committing and pushing. The core is to upload render out the html file, which can help GitHub better build and deploy web pages\nWhen embedding Xaringan into html. Xaringan slides needs to be published first, and in the process, Rmd should preferably be saved as index.rmd. In this way, there will be no bug that can’t find index during build and deployment. Although it was mentioned in class that it is best not to change index.qmd for qmd book, Xaringan has the same requirement\nAfter the push, you need to wait for the build and deployment to be completed before the page is updated. Don’t check a web page and think something’s wrong because it hasn’t been updated. GitHub needs to be given a little time to build and deploy the page\n\n\n\n\nGitHub pages build and deployment"
  },
  {
    "objectID": "Week02.html#reflection",
    "href": "Week02.html#reflection",
    "title": "Week 02 Portfolio",
    "section": "3. Reflection",
    "text": "3. Reflection\nJust like learning Python, the hardest part is configuring the environment. Even though I’ve touched markdown in CASA0013 last term, how to change qmd from local to online in GitHub’s pages is a brand new challenge. As I shared in the Bugs section (1.2. and 2.2.), there were always different issues at the very beginning. index is so important; docs must be named correctly as the default folder; I have to render before updating the QMD and docs folder; and I have to wait for the page to be updated in the GitHub deploy.\nThere were always different challenges, but thankfully the information on Slack was great in helping me embed Xaringan into qmd without any problems, and Google is a great search engine for linking to various official docs or blogs. In addition, using ChatGPT wisely can be a huge boost to efficiency. Although the content it provides may not be accurate, and we need to double-check. However, a general framework and concepts help me to know and understand to a certain extent.\nI am more proficient in the process than I was before using markdown. And this time, I learnt about yml’s declaration of the overall site configuration and the relationship between qmd files. After constant debugging and redeployment, I began to feel that I had mastered the basics of using this tool and was able to layout and update content with relative proficiency.\nNot only do these tools help me with coursework and the learning dairy, but I think I can still use the tools in the future when I want to document something and keep it up to date. Not only are they valuable now, but they will help in the future."
  },
  {
    "objectID": "Week01.html#summary",
    "href": "Week01.html#summary",
    "title": "Week 01 Introduction to remote sensing",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W1 summary\n\n\n\n1.1. Different Sensors\n\n\n\n\n\n\n\n\n\n\n\nEnergy\nReceive\nE.g.,\nApplications\n\n\n\n\nPassive\n❌\nReflected energy from sun (electromagnetic)\nHuman eye, Satellite sensor\nRemote Sensing, meteorological observation, astronomy\n\n\nActive\n✅\nElectromagnetic that actively emit\nRadar, X-ray, LiDAR\nranging, velocity measurement, terrain mapping, and target identification\n\n\n\n Source: NASA\nPassive sensors are more widely used in remote sensing of the Earth and environmental monitoring because they can provide multi-band information on a global scale.\n\nThese sensors do not need to actively send signals to the target, but rely on natural radiation, thus enabling energy-efficient, wide-area observations. For this reason, passive sensors are often mounted on satellites to enable remote sensing observations on a global scale.\nIn addition, passive sensors typically cover multiple wavelength bands, including visible, infrared, and ultraviolet. This broad-spectrum coverage allows them to capture a wide range of features and information about the Earth’s surface, from vegetation cover to surface temperature, in different wavelength bands.\n\n\n\n1.2. Different Orbits\n\n\n\n\n\n\n\n\n\n\nDefinition\nE.g.,\nApplications\n\n\n\n\nGeosynchronous Orbit (GSO)\nSatellite’s period matches the Earth’s rotation period, making the satellite’s position relatively fixed above the Earth\nThe GOES series (Geostationary Operational Environmental Satellites) by the United States and the Meteosat series by Europe\nWeather Forecasting, Climate Monitoring, and communications\n\n\nGeostationary Orbit (GEO)\nSatellite’s speed matches the Earth’s rotation, allowing it to remain stationary relative to a point on the Earth’s surface.\nThe International Telecommunications Satellite Organization, SES (Société Européenne des Satellites)\nCommunication, Observing Earth’s weather, environment, and climate changes, providing valuable remote sensing data\n\n\n\n\n\n1.3. Different Interactions\n\n1.3.1. Earth’s Surface\n\nEnergy will be obsorbed by the surface\nEnergy being transmitted through the surface\n\n Source:Ghimire, 2021\n\nBidirectional Reflectance Distribution Function (BRDF):\nDefinition: A two-dimensional function used to represent the intensity distribution of light entering a material or surface and then exiting the light at the other end.\nApplication: Simulates real-life light conditions, detects particle concentrations in the atmosphere, and detects air humidity in the soil.\nReactions that occur when interacting with the Earth’s surface：\nPolarization: When electromagnetic waves are affected by interference, scattering or reflection, the direction of vibration changes. It is often used to distinguish between different material properties because different types of materials have different polarization responses.\nFluorescence: A substance is excited to emit light that has a longer wavelength. It can usually be used to detect certain specific compounds or biological processes, and is therefore used in fields such as environmental pollution detection and soil characterisation.\n\n\n\n1.3.2. Atmosphere\nAtmospheric scattering\n Source:Bovensmann et al., 2011\nQ: Why is the sky blue (provided that air particles)\nA: Because when the sun is overhead, the shorter wavelength of blue light (as shown in the figure below) is more easily scattered, and thus the sky is observed to be blue. Whereas at sunset, the blue light is scattered away by the atmosphere and the orange-red light passes through the atmosphere into the eye, hence the sky is orange-red.\n Source: The University of Waikato Te Whare Wānanga o Waikato\nSimilarly, seawater has the ability to absorb and scatter. What we usually observe is blue light scattered by seawater.\nIn addition, black colour indicates that all light is absorbed and there is no reflection, scattering or projection. Therefore, the sea floor is black without light.\nWhite, on the other hand, is the result of all colours being absorbed in equal amounts and is the result of mixing various colours together.\n\n\n\n1.4. Data Format\nMajority of remotely sensed data is Raster which is built from pixel. I think this link analysing the difference between a Raster and a Vector is still quite well written and explains what a Raster is quite clearly. In addition, this link also explain raster very well. They also have other information like “Raster bands”, “Image and raster data organization” and so on.\n\nSeveral File Types: BIL, BSQ, BIP, GeoTIFF(Most seen). This Link provides a detailed introduction to those kind of files with examples.\nResolution:\n\n\nRadiometric: The amount of information in each pixel.\n\n\n8-bit can hold 0-255 possibilities, 11-bit can hold 0-2047 possibilities. Here it is perhaps more superficially understood that subtle colour differences can or cannot be represented.\nIn addition, different numbers of spectral channels correspond to different spectral ranges measured on different bands. Multiple spectral channels allow the capture of information in different bands of the surface, such as visible light, infrared, etc. For example, AVIRIS has 224 spectral channels.\n\n\nSpatial: The size of the actual area represented by each pixel point.\n\n\nThe higher the resolution, the finer the surface features captured.\n\n Source: NASA\n\nSpectral：Ability to distinguish spectral detail over different bands.\n\n\nHigher spectral resolution indicates that the data has more information in more bands and is able to distinguish between more landmark features and material composition.\nIn data, each band is stored in a dedicated grating layer. Spectral data can be discrete or continuous.\n\n Source: NIREOS\n\nTemporal: Time interval for acquiring data.\n\n\nThe following figure shows the distribution of time intervals versus pixel resolution for different satellites.\n\n Source: ESRI"
  },
  {
    "objectID": "Week01.html#application",
    "href": "Week01.html#application",
    "title": "Week 01 Introduction to remote sensing",
    "section": "2. Application",
    "text": "2. Application\nTo be update"
  },
  {
    "objectID": "Week03.html#summary",
    "href": "Week03.html#summary",
    "title": "Week 03 Remote sensing data",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W3 Summary\n\n\n\n1.1. Corrections\n\n1.1.1. Geometric\n\nDefinition\nWhen remote sensing data are collected, the sensor is not pointing directly down to the ground; elevation changes due to the undulations of the terrain; changes in the trajectory of the vehicle due to winds; and the relative motion of the satellite with respect to the earth can all lead to image distortion. Correction for such images is geometric correction.\n\n\nSolution\n\n\n\nWorkflow of Geometric Correction\n\n\n\nIn accurate matching of remote sensing images, more complex nonlinear transformation models may be chosen.\nMuch like the GIS application of overlaying old maps into modern GIS. Geo-matching process to ensure accuracy and spatial consistency of maps.\n\n\n\n\n1.1.2. Atmospheric\n\nDefinition\nAtmospheric scattering and topographic attenuation both cause environmental attenuation; in addition, particles and gases in the atmosphere absorb and scatter light, which can create haze that reduces contrast and sharpness in an image. Radiation produces an “adjacency effect” that causes pixels in an image to be affected by surrounding pixels, which affects sharpness. The correction for this type of image is Atmospheric correction.\nRegarding the question on page P26 unnecessary: Why is atmospheric correction not necessary for all such images?\nAnswer: Actually it depends largely on the data and the purpose. If it is just some simple classification task, and highly accurate spectral information is not needed, then there is no need to do Atmospheric correction. Or there is no way to do atmospheric correction when time and resources are limited. Or maybe some data have already been atmospherically corrected, for example, both USGS Landsat and ESA Sentinel mention on their official websites that they use atmospheric correction to process the data.\n\n\nSolution\nThere are 6 methods of atmospheric correction provided in the lecture, and the compiled notes are as follows. My preference is the fifth one because most of the functions or functions can be implemented directly by calling them. With the development of python, the Py6S library makes it easier to use atmospheric radiative transfer models including MODTRAN 4+ and Second Simulation of the Satellite Signal in the Solar Spectrum. The sixth is the most interesting to me, and appears to allow measurements to be made with a handheld device, and then linear relationships to be established to correct the remotely sensed data.\nIt is worth noting, however, that when comparing data before and after calibration, it is necessary to ensure that the scales are consistent (e.g., P29).\n\n\n\nAtmospheric Solution Types\n\n\n\n\n\n1.1.3. Topographic (Orthorectification)\n\n\n\nOrthorectification / Topographic Correction Overview\n\n\n\nDefinition\nOrthorectification correction is a subset of geometric correction, where the image is distorted because the sensor is not pointing directly below the ground. The correction to remove the distortion in the image is Orthorectification correction.\n\n\nSolution\nSensor geometry information including sensor position, attitude and viewing angle is required to determine the actual position of the pixel on the ground. The terrain variations are then taken into account in conjunction with the Digital Elevation Model (DEM) to correct image distortions.\nIn searching for the Digital Elevation Model, I found that Orthorectification correction is only one direction of its application. At its core, it still contains information on topographic relief, slope and aspect, which can be used not only for remote sensing data correction, but also for modelling water flow paths, flood risk areas and hydrological processes. Its applications are covered in the next “2. Application” section.\n\n\n\n1.1.4. Radiometric\n\n\n\nRadiometric Correction Overview\n\n\n\nDefinition\nThe radiance of image data can be inconsistent across time, location, and sensor, Radiometric Calibration calibrates the response of a sensor by converting the digital number count (DN) of an image into a physically meaningful radiance or emissivity value.\n\n\nSolution\n\n\n\nWorkflow of Radiometric Correction\n\n\n\nBias is a constant that represents the zero offset of the sensor. It is used to adjust the output of the sensor to ensure that the zero drift of the sensor is taken into account in the radiance calculation.\nGain is a scaling factor that is used to convert a digital number (DN) to an actual radiance value. It represents the increase in radiance corresponding to each digital count value (DN) to convert to true radiance.\n\n\n\n\n\n1.2. Image Joining\n\n\n\nFeathering Overview\n\n\n\nIn data joining for remote sensing, Mosaicking and Feathering are more like an assembly line. First, Mosaicking stitches multiple remote sensing images together to form a continuous, seamless whole. Then Feathering creates smooth transitions in the overlapping areas.\nFeathering works by reducing the impact of one image while increasing the impact of another at the boundaries, which may be achieved by weighted averaging of pixel values, modifying the transparency or opacity of the alpha channel, or gradient blending techniques.\n\n\n\n1.3. Image Enhancement\n\n\n\nEnhancement Methods Overview\n\n\nThe class briefly went over 7 methods of image enhancement, each with different scenarios of application. Just like photo retouching, different parameters (contrast, sharpening, texture) focus on different parts. The best ones for me to understand are Contrast Enhancement, Band Ratioing, Edge Enhancement, and Texture, because their purpose is clear: to enhance the details in an image or to highlight certain parts of a feature (Maybe because of the extensive photo retouching, it is easier to understand how the methods for highlighting image features work hahahahahahahahaha). The most difficult one is PCA, which I once remembered from my undergraduate algorithms class, and I will try my best to cover its different applications in the “Application” section."
  },
  {
    "objectID": "Week03.html#application",
    "href": "Week03.html#application",
    "title": "Week 03 Remote sensing data",
    "section": "2. Application",
    "text": "2. Application"
  },
  {
    "objectID": "Week03.html#reflecion",
    "href": "Week03.html#reflecion",
    "title": "Week 03 Remote sensing data",
    "section": "3. Reflecion",
    "text": "3. Reflecion"
  },
  {
    "objectID": "Week04.html#summary",
    "href": "Week04.html#summary",
    "title": "Week 04 SAR applications & Policies",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W4 Summary\n\n\n\n1.1. Synthetic Aperture Radar (SAR)\n\n1.1.1. Background info\nSynthetic Aperture Radar, or SAR for short, is a type of active sensor. Unlike passive remote sensing, active sensors are not affected by weather, cloud cover, or light. The time delay and phase difference of the reflected signals are recorded by actively transmitting microwave pulses to generate surface images.\nIn addition to being unaffected by weather and light, SAR is also highly penetrating and can penetrate vegetation (mentioned in the Wavelength section).\n\n\n1.1.2. Polarization\nThe concept of polarization was mentioned in Week1. However, in Week4, different types of polarization are introduced, such as horizontal polarization (HH), vertical polarization (VV), and cross-polarization (HV).\n Source: Dabboor and Brisco, 2018\nDifferent types of features have different reflection and scattering characteristics of radar signals. For example, bare rock or ground is more sensitive to vertical polarization and scatters in the same direction as the vertical polarization, making it easier to capture this type of signal; leaves and branches in vegetation respond differently to radar waves, resulting in signals that vary in different polarization modes, and are therefore more sensitive to cross-polarization; in vertical structures, such as trees and buildings, the radar wave is first in contact with the ground, and then reflected again to the receiver, and horizontal polarization is the most sensitive for such cases.\n\n\n1.1.3. Wavelength\nDifferent wavelengths have different usage scenarios, for example, radar systems with shorter wavelengths have higher resolution and are able to display features more clearly. However, the penetration ability for atmosphere and vegetation is poor (Ottinger and Kuenzer, 2020), and it is not easy to obtain information on the surface obscured by vegetation.\n Source: Ottinger and Kuenzer, 2020\nRadar systems of different wavelengths respond differently to polarised scattering, e.g. more scattering is produced by features on rough surfaces (The SAR Handbook).\n Source:Ottinger and Kuenzer, 2020\n\n\n\n1.2. Applications of SAR\n\nPhase and Amplitude Data: The SAR actively transmitting wave records information by scattering from the target, which is recorded in two main ways: phase and amplitude. Phase is used to determine the distance from the sensor to the target, while amplitude indicates the amount of transmitted signal returned to the sensor. Amplitude and phase measurements provide information about the roughness, geometry, and humidity of the target.\n\nSAR has many applications, such as monitoring forests and illegal logging (Hansen et al., 2010), drought monitoring, forest fire monitoring (Morante-Carballo et al., 2022), and * temperature and heat island effect research in P48-P50 (Li et al., 2022) (which also will be mentioned in Week5).\n\nInterferometric Synthetic Aperture Radar (InSAR): SAR-based extensions use the phase difference between two or more radar images to derive elevation information about the surface. It is mainly used in topographic surveys and surface deformation monitoring (e.g., earthquakes) Data processing for InSAR involves interferometric image generation, creation of elevation models, etc.\n\n\n\n1.3. Polocies\n\n\n\n\nPolicies\n\n\n\n\nGlobal\nNew Urban Agenda, Sustainable Development Goals\n\n\nMetropolitan\nPolicy SI 12 Flood risk management, OneNYC 2025\n\n\nLocal\nCape Town Municipal Spatial Development Framework\n\n\n\nPolicies at the global level consist mainly of the New Urban Agenda and the SDGs, both of which I believe emphasise the core concept of sustainability. The New Urban Agenda stresses how to build an inclusive, safe, resilient and sustainable city, as do the Sustainable Development Goals. In other words, ensuring that urban development meets human needs without negatively impacting the environment and society. But the literature doesn’t seem to be able to integrate policies with human needs very well, and the question of Page 53 is well worth thinking about.\nPolicies, whether metropolitan or local, are considered to be detailed to a particular city. Global policies are used more to monitor and understand the changing conditions of the earth as a whole, while local policies are more local and specific, more directly affecting the lives of citizens and the sustainable development of cities. Policies in these cities are mainly concerned with disaster monitoring, land use, environmental monitoring, and post-disaster reconstruction, and remote sensing can provide data and information in this part. For example, remote sensing data combined with models and algorithms can be used to predict and simulate environmental phenomena, thus helping policy makers to visualise the impact of different policies and providing them with decision support."
  },
  {
    "objectID": "Week04.html#application",
    "href": "Week04.html#application",
    "title": "Week 04 SAR applications & Policies",
    "section": "2. Application",
    "text": "2. Application\n\n2.1. Introduction\nI have chosen Mumbai as the city for my research, which is located in the western Indian state of Maharashtra. As one of the largest cities in India, Mumbai is the economic, financial and commercial centre of India and one of the busiest ports in the country. Since the Air Quality Index (AQI) of Mumbai has been maintained at unhealthy levels for a long time, while there are many news reports about Mumbai’s poor air quality and the failure of construction sites to be regulated, I would like to study India whether the Mumbai Municipal Corporation has enacted some policies to control air pollution. In addition, can remote sensing data help government workers to formulate better policies to protect the environment in which people live.\n\n\n2.2. Background\nAccording to the historical data of Air Quality Index of AQI.IN, Mumbai is rarely able to achieve Good air quality in the year 2023, while London is almost the exact opposite.\n Source:AQI.IN  Source:AQI.IN\nAccording to The Time of India reports in 2022 and 2023, air pollution continues to be a serious problem. And according to reports, PM2.5 in the homes of people living near construction sites exceeds 300µg/m3, which is even more than three times the official figure (69.5µg/m3), compared to the UN’s healthy range of &lt;10µg/m3.According to CARB’s information, PM2.5 can have adverse health effects, including premature death, bronchitis, asthma attacks, respiratory symptoms, and debilitating lung function development in children are associated.\n\n\n2.3. Policies\n\n2.3.1. Global\nTo allow air pollution to become so severe is contrary to commitments (b), (c), and (d) of Article 4 of The United Nations Framework Convention on Climate Change (UNFCCC); 65, 75, and 79 of The New Urban Agenda; “Goal 3, 11, 13 of The Sustainable Development Goals.\n Source:UNFCCC, The New Urban Agenda, SDGs\nOverall, Mumbai’s current unsustainable building patterns in the city, greenhouse gas emissions that far exceed standards, and particulate emissions that are harmful to people’s health are all contrary to global policies. Malathy Iyer’s (2023) research suggests that Mumbai’s air pollution stems not only from unsustainable construction modes, but also from construction-induced reductions in road surfaces, combined with Mumbai’s over 120,000 car ownership. Vehicles need to spend more time on the road, which leads to an increase in NOx and pollutant emissions. In addition, NOx generates oxygen radicals in a photochemical reaction, which combine with oxygen molecules to generate abrasive and corrosive ozone, further harming people’s health.\n\n\n2.3.2. National and Local\n  Source: Mumbai Climate Action Plan 2022\n Source: Government of India\nPolicies at the Metropolitan and Local levels are more focused on practical implementation due to the contribution of total suspended particulate matter (PM) emissions from construction activities and pollution from traffic exhaust. The emphasis is on safeguarding the health of citizens through sustainable transport and air quality management. Mumbai Climate Action Plan 2022 states that reliance on ground-based monitoring stations limits the scope and breadth of analyses, and thus fails to identify past patterns and future trends. Remotely sensed data can go some way towards filling critical data gaps.\n\n\n\n2.4. Appling Remote Sense Data and Methods to Achieve Policy Goals\nAccording to Lim et al. (2009), their proposed algorithm is able to calculate PM10 concentrations using remotely sensed data and allows for better visualisation of mapping PM10 concentrations. The source of the data is Landsat TM 5 and according to their proposed algorithm it was found that there is a linear relationship between Atmosphere Reflectance and the change in PM10:\n\\[\nR_{atm}(\\lambda) = \\frac{S}{4\\mu_s\\mu_v} [\\sigma_aP_a(\\Theta, \\lambda) + \\sigma_rP_r(\\Theta, \\lambda)]\n\\]\nIn addition, Scheibenreif, Mommert, and Borth (2021) used deep learning models to predict ambient air pollution. Their use of optical satellite imagery and remote sensing data (Sentinel-2 Level-2A data) avoids the local and temporal limitations of traditional data. And they are able to zoom in on air pollution estimates at high spatial resolution to arbitrary locations, identify major sources of air pollution and greenhouse gases, and monitor them over time.\n\n\n2.5. Reflection of Applications\nBoth studies contribute to the fulfilment of policy goals or the solution of problems, and the application of remotely sensed data allows researchers to have a more dynamic and complete view of climate and air quality. In the former, mathematical methods and regression algorithms were used to demonstrate that PM10 concentrations can be calculated from atmospheric reflectance, which, however, is related to top-of-atmosphere reflectance and surface reflectance. Although the theoretical additive and subtractive relationships are stated, the errors in the tests are not taken into account. The latter is the prediction of pollutant concentration by machine learning algorithms, unfortunately due to the limitation of the dataset they were only able to obtain an R2 of 0.25. Although the pre-training provided by the migration learning improved the model’s performance to a certain extent (R2=0.45), the MAE was still 6.62±0.17 μg/m3, which means that the model still did not have a very good performance. However, as the field of deep learning develops, more powerful models can hopefully provide more accurate pollution predictions to help managers create sustainable cities and improve the health of citizens."
  },
  {
    "objectID": "Week04.html#reflection",
    "href": "Week04.html#reflection",
    "title": "Week 04 SAR applications & Policies",
    "section": "3. Reflection",
    "text": "3. Reflection\nThis week’s class is relatively theoretical, and after introduction of first week, the concepts and workings related to SAR are well understood. But InSAR is still a challenging concept and particular application of SAR. But maybe not every application needs to be very clear about every detail, mastering the overall process as well as the overall functionality is enough for me to use and search for related detailed applications. The Case provided by P48-P50 is also very helpful, combining the classic framework of the thesis, and showing how to use remote sensing as a data source and technology in solving practical problems in the section of Data and Methodology.In fact, remote sensing data is not a very complicated and difficult concept, and this case largely reduced my unfamiliarity with the concept of remote sensing.\nRegarding the part about policies and remote sensing, I had a question when I was listening to the lecture: what is the relationship between these policies and remote sensing? After doing some searching, I found that this relationship can be in two parts. The first part is “pre-policy”, where remote sensing data can be used in conjunction with models and algorithms to visualise different outcomes for planners before a policy is issued, helping them to decide how the policy should be made. The other part is “post-policy”, where policies have been issued in response to specific phenomena (e.g., heat, floods, droughts, etc.) due to the presence of those phenomena. Remote sensing can be used as a technology to help governments and organisations to better understand and respond to environmental challenges. Supporting them at the data level to make the best use of resources.\nOne of the great things about this lecture is that it made me realise that not every detail is so important, and that it is much clearer to have a general framework and process in place first. So many of the policies in the second half of the class were still a little vague for me, despite being broken down into Global, Metropolitan, and Local. However, when I really studied them in detail, and read the policies in detail, I realised that I could actually understand them in a different way: global policies provide the general framework, and urban policies are implemented in different aspects of urban management with remote sensing data in order to fulfil the vision of the global policies.\n\n\n\nConnection within Policies in my opinion"
  }
]