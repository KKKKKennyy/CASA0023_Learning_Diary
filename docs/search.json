[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "LearningDiary",
    "section": "",
    "text": "Preface\nThis is a Learning Diary of CASA0023 Remote Sensing."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "LearningDiary",
    "section": "About me",
    "text": "About me\n\n\n\nyy’s portrait\n\n\nKKKKKenny,"
  },
  {
    "objectID": "index.html#about-layout-of-learning-diary",
    "href": "index.html#about-layout-of-learning-diary",
    "title": "LearningDiary",
    "section": "About Layout of Learning Diary",
    "text": "About Layout of Learning Diary\nYou can also click bar in the left to navigate to corresponding page\n\nWeek 01\nWeek 02\nWeek 03"
  },
  {
    "objectID": "Week01.html",
    "href": "Week01.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Week02.html",
    "href": "Week02.html",
    "title": "Week 02 Portfolio",
    "section": "",
    "text": "If that preview doesn’t work, please click Link to find out more."
  },
  {
    "objectID": "Week03.html",
    "href": "Week03.html",
    "title": "References",
    "section": "",
    "text": "Knuth, Donald E. 1984. “Literate Programming.” Comput.\nJ. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "Week01.html#theoretical-part",
    "href": "Week01.html#theoretical-part",
    "title": "Week01 Introduction to remote sensing",
    "section": "1. Theoretical Part",
    "text": "1. Theoretical Part\n\n\n\nMindmap for W1 lecture part\n\n\n\n1.1. Different Sensors\n\n\n\n\n\n\n\n\n\n\n\nEnergy\nReceive\nE.g.,\nApplications\n\n\n\n\nPassive\n❌\nReflected energy from sun (electromagnetic)\nHuman eye, Satellite sensor\nRemote Sensing, meteorological observation, astronomy\n\n\nActive\n✅\nElectromagnetic that actively emit\nRadar, X-ray, LiDAR\nranging, velocity measurement, terrain mapping, and target identification\n\n\n\n Source: NASA\nPassive sensors are more widely used in remote sensing of the Earth and environmental monitoring because they can provide multi-band information on a global scale.\n\nThese sensors do not need to actively send signals to the target, but rely on natural radiation, thus enabling energy-efficient, wide-area observations. For this reason, passive sensors are often mounted on satellites to enable remote sensing observations on a global scale.\nIn addition, passive sensors typically cover multiple wavelength bands, including visible, infrared, and ultraviolet. This broad-spectrum coverage allows them to capture a wide range of features and information about the Earth’s surface, from vegetation cover to surface temperature, in different wavelength bands.\n\n\n\n1.2. Different Orbits\n\n\n\n\n\n\n\n\n\n\nDefinition\nE.g.,\nApplications\n\n\n\n\nGeosynchronous Orbit (GSO)\nSatellite’s period matches the Earth’s rotation period, making the satellite’s position relatively fixed above the Earth\nThe GOES series (Geostationary Operational Environmental Satellites) by the United States and the Meteosat series by Europe\nWeather Forecasting, Climate Monitoring, and communications\n\n\nGeostationary Orbit (GEO)\nSatellite’s speed matches the Earth’s rotation, allowing it to remain stationary relative to a point on the Earth’s surface.\nThe International Telecommunications Satellite Organization, SES (Société Européenne des Satellites)\nCommunication, Observing Earth’s weather, environment, and climate changes, providing valuable remote sensing data\n\n\n\n\n\n1.3. Different Interactions\n\n1.3.1. Earth’s Surface\n\nEnergy will be obsorbed by the surface\nEnergy being transmitted through the surface\n\n Source:Ghimire, 2021\n\nBidirectional Reflectance Distribution Function (BRDF):\nDefinition: A two-dimensional function used to represent the intensity distribution of light entering a material or surface and then exiting the light at the other end.\nApplication: Simulates real-life light conditions, detects particle concentrations in the atmosphere, and detects air humidity in the soil.\nReactions that occur when interacting with the Earth’s surface：\nPolarization: When electromagnetic waves are affected by interference, scattering or reflection, the direction of vibration changes. It is often used to distinguish between different material properties because different types of materials have different polarization responses.\nFluorescence: A substance is excited to emit light that has a longer wavelength. It can usually be used to detect certain specific compounds or biological processes, and is therefore used in fields such as environmental pollution detection and soil characterisation.\n\n\n\n1.3.2. Atmosphere\nAtmospheric scattering\n Source:Bovensmann et al., 2011\nQ: Why is the sky blue (provided that air particles)\nA: Because when the sun is overhead, the shorter wavelength of blue light (as shown in the figure below) is more easily scattered, and thus the sky is observed to be blue. Whereas at sunset, the blue light is scattered away by the atmosphere and the orange-red light passes through the atmosphere into the eye, hence the sky is orange-red.\n Source: The University of Waikato Te Whare Wānanga o Waikato\nSimilarly, seawater has the ability to absorb and scatter. What we usually observe is blue light scattered by seawater.\nIn addition, black colour indicates that all light is absorbed and there is no reflection, scattering or projection. Therefore, the sea floor is black without light.\nWhite, on the other hand, is the result of all colours being absorbed in equal amounts and is the result of mixing various colours together.\n\n\n\n1.4. Data Format\nMajority of remotely sensed data is Raster which is built from pixel. I think this link analysing the difference between a Raster and a Vector is still quite well written and explains what a Raster is quite clearly. In addition, this link also explain raster very well. They also have other information like “Raster bands”, “Image and raster data organization” and so on.\n\nSeveral File Types: BIL, BSQ, BIP, GeoTIFF(Most seen). This Link provides a detailed introduction to those kind of files with examples.\nResolution:\n\n\nRadiometric: The amount of information in each pixel.\n\n\n8-bit can hold 0-255 possibilities, 11-bit can hold 0-2047 possibilities. Here it is perhaps more superficially understood that subtle colour differences can or cannot be represented.\nIn addition, different numbers of spectral channels correspond to different spectral ranges measured on different bands. Multiple spectral channels allow the capture of information in different bands of the surface, such as visible light, infrared, etc. For example, AVIRIS has 224 spectral channels.\n\n\nSpatial: The size of the actual area represented by each pixel point.\n\n\nThe higher the resolution, the finer the surface features captured.\n\n Source: NASA\n\nSpectral：Ability to distinguish spectral detail over different bands.\n\n\nHigher spectral resolution indicates that the data has more information in more bands and is able to distinguish between more landmark features and material composition.\nIn data, each band is stored in a dedicated grating layer. Spectral data can be discrete or continuous.\n\n Source: NIREOS\n\nTemporal: Time interval for acquiring data.\n\n\nThe following figure shows the distribution of time intervals versus pixel resolution for different satellites.\n\n Source: ESRI"
  },
  {
    "objectID": "Week01.html#practical-part",
    "href": "Week01.html#practical-part",
    "title": "Week01 Introduction to remote sensing",
    "section": "Practical Part",
    "text": "Practical Part\n(Waiting for updating)"
  },
  {
    "objectID": "Week01.html#what-i-learned-from-mistakes.",
    "href": "Week01.html#what-i-learned-from-mistakes.",
    "title": "Week01 Introduction to remote sensing",
    "section": "What I learned from mistakes.",
    "text": "What I learned from mistakes.\n(Waiting for updating)"
  },
  {
    "objectID": "Week01.html#reflection",
    "href": "Week01.html#reflection",
    "title": "Week 01 Introduction to remote sensing",
    "section": "3. Reflection",
    "text": "3. Reflection\nThe first week was a relatively basic introductory class, so overall it wasn’t too difficult to get a basic knowledge of an unfamiliar field.\nAlthough I had forgotten a bit about the physics when it came to wavelengths and frequencies, it was good to see that only some concepts were covered. The subsequent understanding and working principle are not affected.\nIn addition, the storage and processing of remote sensing data (raster) in class was also very confusing. But the good thing is that the Drop-in session immediately following was able to talk about and solve this problem. Each pixel point has a unique spectral map (if it’s a multi-band image), usually synthesised from Red, Green, and Blue. But it is also possible to choose different wavelengths for the synthesis, depending on the needs of the study.\nI’m thankful to ArcGIS, ESRI, and NASA for really providing a lot of valid information in their offical documents to help me make sense of it. For example, this link is a detailed description of the Raster data. Also thanks to ChatGPT for being able to give me some basic Q&A’s."
  },
  {
    "objectID": "Week02.html#xaringan",
    "href": "Week02.html#xaringan",
    "title": "Week 02 Portfolio",
    "section": "1. Xaringan",
    "text": "1. Xaringan\n\n1.1. Xaringan Sample\nXaringan is a new tool for me, it’s a package in R. It can knit rmd into a html for presentation, at first I thought it’s a hard thing to do, but actually with the help of css it can have a lot of functions and styles.And it is very clear to use ‘—’ to separate different pages.\nXaringan Sample\n\n\n1.2. Bugs I’ve encountered in using Xaringan for Slides (FYI)\n\nBoth Xaringan and XaringanExtra require remote installation, and the instructions need to be executed on Console instead of Terminal\nWhen the template file is created, the DT package from CARN should also be installed\nUnlike yml documents, ‘—’ does not need to appear in pairs in Xaringan\n\n\noption(htmltools.dir.version = FALSE)\n\n\nAvoid the unconcise directory structure that comes with versioning\nIn the header of the Rmd file, several CSS files, which will be invoked in sequence in Knitting, can be declared at the same time\nIf you want the entire slides font to be adjusted, you should declare this in your CSS file. The font declared in the header can only affect the font of the current page\nSetting eval = FALSE at the beginning of a code block allows the code not to run, but to be displayed on the web page. Setting echo = FALSE can display only the result of running the code without showing the code\n\n\n\n1.3. Useful Materials\n\nXaringan Template in RStudio\nFonts, Colors, Theme Settings"
  },
  {
    "objectID": "Week02.html#quarto-mark-down",
    "href": "Week02.html#quarto-mark-down",
    "title": "Week 02 Portfolio",
    "section": "2. Quarto Mark Down",
    "text": "2. Quarto Mark Down\nQuarto Markdown is an upgraded version of R markdown. qmd document can be rendered into html or pdf or other formats.\nCompared to HTML, qmd is much simpler and doesn’t require pairs of ‘&lt;&gt;’ to declare styles, just a few symbols (like *, _, #) to create some styles.\nIn addition, images can be inserted by providing a file path or link, and by default they fill the screen without worrying about placement.\n\n2.1. Embedded Xaringan in QMD\nIf that preview doesn’t work, please click Link to find out more.\n\n\n\n\n\n\n\n\n\n\n2.2. Bugs I’ve encountered in using QMD (FYI)\n\nYou should first declare “output-dir: docs” in the yml file, which is consistent with the default setting in GitHub pages deployment. Remember not to type docs as docx, or you will get an error in the GitHub Action section.\nAll changes in Rstudio must render before committing and pushing. The core is to upload render out the html file, which can help GitHub better build and deploy web pages\nWhen embedding Xaringan into html. Xaringan slides needs to be published first, and in the process, Rmd should preferably be saved as index.rmd. In this way, there will be no bug that can’t find index during build and deployment. Although it was mentioned in class that it is best not to change index.qmd for qmd book, Xaringan has the same requirement\nAfter the push, you need to wait for the build and deployment to be completed before the page is updated. Don’t check a web page and think something’s wrong because it hasn’t been updated. GitHub needs to be given a little time to build and deploy the page\n\n\n\n\nGitHub pages build and deployment"
  },
  {
    "objectID": "Week02.html#reflection",
    "href": "Week02.html#reflection",
    "title": "Week 02 Portfolio",
    "section": "3. Reflection",
    "text": "3. Reflection\nJust like learning Python, the hardest part is configuring the environment. Even though I’ve touched markdown in CASA0013 last term, how to change qmd from local to online in GitHub’s pages is a brand new challenge. As I shared in the Bugs section (1.2. and 2.2.), there were always different issues at the very beginning. index is so important; docs must be named correctly as the default folder; I have to render before updating the QMD and docs folder; and I have to wait for the page to be updated in the GitHub deploy.\nThere were always different challenges, but thankfully the information on Slack was great in helping me embed Xaringan into qmd without any problems, and Google is a great search engine for linking to various official docs or blogs. In addition, using ChatGPT wisely can be a huge boost to efficiency. Although the content it provides may not be accurate, and we need to double-check. However, a general framework and concepts help me to know and understand to a certain extent.\nI am more proficient in the process than I was before using markdown. And this time, I learnt about yml’s declaration of the overall site configuration and the relationship between qmd files. After constant debugging and redeployment, I began to feel that I had mastered the basics of using this tool and was able to layout and update content with relative proficiency.\nNot only do these tools help me with coursework and the learning dairy, but I think I can still use the tools in the future when I want to document something and keep it up to date. Not only are they valuable now, but they will help in the future."
  },
  {
    "objectID": "Week01.html#summary",
    "href": "Week01.html#summary",
    "title": "Week 01 Introduction to remote sensing",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W1 summary\n\n\n\n1.1. Different Sensors\n\n\n\n\n\n\n\n\n\n\n\nEnergy\nReceive\nE.g.,\nApplications\n\n\n\n\nPassive\n❌\nReflected energy from sun (electromagnetic)\nHuman eye, Satellite sensor\nRemote Sensing, meteorological observation, astronomy\n\n\nActive\n✅\nElectromagnetic that actively emit\nRadar, X-ray, LiDAR\nranging, velocity measurement, terrain mapping, and target identification\n\n\n\n Source: NASA\nPassive sensors are more widely used in remote sensing of the Earth and environmental monitoring because they can provide multi-band information on a global scale.\n\nThese sensors do not need to actively send signals to the target, but rely on natural radiation, thus enabling energy-efficient, wide-area observations. For this reason, passive sensors are often mounted on satellites to enable remote sensing observations on a global scale.\nIn addition, passive sensors typically cover multiple wavelength bands, including visible, infrared, and ultraviolet. This broad-spectrum coverage allows them to capture a wide range of features and information about the Earth’s surface, from vegetation cover to surface temperature, in different wavelength bands.\n\n\n\n1.2. Different Orbits\n\n\n\n\n\n\n\n\n\n\nDefinition\nE.g.,\nApplications\n\n\n\n\nGeosynchronous Orbit (GSO)\nSatellite’s period matches the Earth’s rotation period, making the satellite’s position relatively fixed above the Earth\nThe GOES series (Geostationary Operational Environmental Satellites) by the United States and the Meteosat series by Europe\nWeather Forecasting, Climate Monitoring, and communications\n\n\nGeostationary Orbit (GEO)\nSatellite’s speed matches the Earth’s rotation, allowing it to remain stationary relative to a point on the Earth’s surface.\nThe International Telecommunications Satellite Organization, SES (Société Européenne des Satellites)\nCommunication, Observing Earth’s weather, environment, and climate changes, providing valuable remote sensing data\n\n\n\n\n\n1.3. Different Interactions\n\n1.3.1. Earth’s Surface\n\nEnergy will be obsorbed by the surface\nEnergy being transmitted through the surface\n\n Source:Ghimire, 2021\n\nBidirectional Reflectance Distribution Function (BRDF):\nDefinition: A two-dimensional function used to represent the intensity distribution of light entering a material or surface and then exiting the light at the other end.\nApplication: Simulates real-life light conditions, detects particle concentrations in the atmosphere, and detects air humidity in the soil.\nReactions that occur when interacting with the Earth’s surface：\nPolarization: When electromagnetic waves are affected by interference, scattering or reflection, the direction of vibration changes. It is often used to distinguish between different material properties because different types of materials have different polarization responses.\nFluorescence: A substance is excited to emit light that has a longer wavelength. It can usually be used to detect certain specific compounds or biological processes, and is therefore used in fields such as environmental pollution detection and soil characterisation.\n\n\n\n1.3.2. Atmosphere\nAtmospheric scattering\n Source:Bovensmann et al., 2011\nQ: Why is the sky blue (provided that air particles)\nA: Because when the sun is overhead, the shorter wavelength of blue light (as shown in the figure below) is more easily scattered, and thus the sky is observed to be blue. Whereas at sunset, the blue light is scattered away by the atmosphere and the orange-red light passes through the atmosphere into the eye, hence the sky is orange-red.\n Source: The University of Waikato Te Whare Wānanga o Waikato\nSimilarly, seawater has the ability to absorb and scatter. What we usually observe is blue light scattered by seawater.\nIn addition, black colour indicates that all light is absorbed and there is no reflection, scattering or projection. Therefore, the sea floor is black without light.\nWhite, on the other hand, is the result of all colours being absorbed in equal amounts and is the result of mixing various colours together.\n\n\n\n1.4. Data Format\nMajority of remotely sensed data is Raster which is built from pixel. I think this link analysing the difference between a Raster and a Vector is still quite well written and explains what a Raster is quite clearly. In addition, this link also explain raster very well. They also have other information like “Raster bands”, “Image and raster data organization” and so on.\n\nSeveral File Types: BIL, BSQ, BIP, GeoTIFF(Most seen). This Link provides a detailed introduction to those kind of files with examples.\nResolution:\n\n\nRadiometric: The amount of information in each pixel.\n\n\n8-bit can hold 0-255 possibilities, 11-bit can hold 0-2047 possibilities. Here it is perhaps more superficially understood that subtle colour differences can or cannot be represented.\nIn addition, different numbers of spectral channels correspond to different spectral ranges measured on different bands. Multiple spectral channels allow the capture of information in different bands of the surface, such as visible light, infrared, etc. For example, AVIRIS has 224 spectral channels.\n\n\nSpatial: The size of the actual area represented by each pixel point.\n\n\nThe higher the resolution, the finer the surface features captured.\n\n Source: NASA\n\nSpectral：Ability to distinguish spectral detail over different bands.\n\n\nHigher spectral resolution indicates that the data has more information in more bands and is able to distinguish between more landmark features and material composition.\nIn data, each band is stored in a dedicated grating layer. Spectral data can be discrete or continuous.\n\n Source: NIREOS\n\nTemporal: Time interval for acquiring data.\n\n\nThe following figure shows the distribution of time intervals versus pixel resolution for different satellites.\n\n Source: ESRI"
  },
  {
    "objectID": "Week01.html#application",
    "href": "Week01.html#application",
    "title": "Week 01 Introduction to remote sensing",
    "section": "2. Application",
    "text": "2. Application\nTo be update"
  },
  {
    "objectID": "Week03.html#summary",
    "href": "Week03.html#summary",
    "title": "Week 03 Remote sensing data",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W3 Summary\n\n\n\n1.1. Corrections\n\n1.1.1. Geometric\n\nDefinition\nWhen remote sensing data are collected, the sensor is not pointing directly down to the ground; elevation changes due to the undulations of the terrain; changes in the trajectory of the vehicle due to winds; and the relative motion of the satellite with respect to the earth can all lead to image distortion. Correction for such images is geometric correction.\n\n\nSolution\n\n\n\nWorkflow of Geometric Correction\n\n\n\nIn accurate matching of remote sensing images, more complex nonlinear transformation models may be chosen.\nMuch like the GIS application of overlaying old maps into modern GIS. Geo-matching process to ensure accuracy and spatial consistency of maps.\n\n\n\n\n1.1.2. Atmospheric\n\nDefinition\nAtmospheric scattering and topographic attenuation both cause environmental attenuation; in addition, particles and gases in the atmosphere absorb and scatter light, which can create haze that reduces contrast and sharpness in an image. Radiation produces an “adjacency effect” that causes pixels in an image to be affected by surrounding pixels, which affects sharpness. The correction for this type of image is Atmospheric correction.\nRegarding the question on page P26 unnecessary: Why is atmospheric correction not necessary for all such images?\nAnswer: Actually it depends largely on the data and the purpose. If it is just some simple classification task, and highly accurate spectral information is not needed, then there is no need to do Atmospheric correction. Or there is no way to do atmospheric correction when time and resources are limited. Or maybe some data have already been atmospherically corrected, for example, both USGS Landsat and ESA Sentinel mention on their official websites that they use atmospheric correction to process the data.\n\n\nSolution\nThere are 6 methods of atmospheric correction provided in the lecture, and the compiled notes are as follows. My preference is the fifth one because most of the functions or functions can be implemented directly by calling them. With the development of python, the Py6S library makes it easier to use atmospheric radiative transfer models including MODTRAN 4+ and Second Simulation of the Satellite Signal in the Solar Spectrum. The sixth is the most interesting to me, and appears to allow measurements to be made with a handheld device, and then linear relationships to be established to correct the remotely sensed data.\nIt is worth noting, however, that when comparing data before and after calibration, it is necessary to ensure that the scales are consistent (e.g., P29).\n\n\n\nAtmospheric Solution Types\n\n\n\n\n\n1.1.3. Topographic (Orthorectification)\n\n\n\nOrthorectification / Topographic Correction Overview\n\n\n\nDefinition\nOrthorectification correction is a subset of geometric correction, where the image is distorted because the sensor is not pointing directly below the ground. The correction to remove the distortion in the image is Orthorectification correction.\n\n\nSolution\nSensor geometry information including sensor position, attitude and viewing angle is required to determine the actual position of the pixel on the ground. The terrain variations are then taken into account in conjunction with the Digital Elevation Model (DEM) to correct image distortions.\nIn searching for the Digital Elevation Model, I found that Orthorectification correction is only one direction of its application. At its core, it still contains information on topographic relief, slope and aspect, which can be used not only for remote sensing data correction, but also for modelling water flow paths, flood risk areas and hydrological processes. Its applications are covered in the next “2. Application” section.\n\n\n\n1.1.4. Radiometric\n\n\n\nRadiometric Correction Overview\n\n\n\nDefinition\nThe radiance of image data can be inconsistent across time, location, and sensor, Radiometric Calibration calibrates the response of a sensor by converting the digital number count (DN) of an image into a physically meaningful radiance or emissivity value.\n\n\nSolution\n\n\n\nWorkflow of Radiometric Correction\n\n\n\nBias is a constant that represents the zero offset of the sensor. It is used to adjust the output of the sensor to ensure that the zero drift of the sensor is taken into account in the radiance calculation.\nGain is a scaling factor that is used to convert a digital number (DN) to an actual radiance value. It represents the increase in radiance corresponding to each digital count value (DN) to convert to true radiance.\n\n\n\n\n\n1.2. Image Joining\n\n\n\nFeathering Overview\n\n\n\nIn data joining for remote sensing, Mosaicking and Feathering are more like an assembly line. First, Mosaicking stitches multiple remote sensing images together to form a continuous, seamless whole. Then Feathering creates smooth transitions in the overlapping areas.\nFeathering works by reducing the impact of one image while increasing the impact of another at the boundaries, which may be achieved by weighted averaging of pixel values, modifying the transparency or opacity of the alpha channel, or gradient blending techniques.\n\n\n\n1.3. Image Enhancement\n\n\n\nEnhancement Methods Overview\n\n\nThe class briefly went over 7 methods of image enhancement, each with different scenarios of application. Just like photo retouching, different parameters (contrast, sharpening, texture) focus on different parts. The best ones for me to understand are Contrast Enhancement, Band Ratioing, Edge Enhancement, and Texture, because their purpose is clear: to enhance the details in an image or to highlight certain parts of a feature (Maybe because of the extensive photo retouching, it is easier to understand how the methods for highlighting image features work hahahahahahahahaha). The most difficult one is PCA, which I once remembered from my undergraduate algorithms class, and I will try my best to cover its different applications in the “Application” section."
  },
  {
    "objectID": "Week03.html#application",
    "href": "Week03.html#application",
    "title": "Week 03 Remote sensing data",
    "section": "2. Application",
    "text": "2. Application\nPrincipal Component Analysis (PCA) is a statistical method used to reveal important features in data by reducing the dimensionality of the data while preserving as much of the variance of the original data as possible.The basic idea behind PCA is to find the main directions of variation in the data and to re-represent the data as coordinates in those directions, which are called principal components.\nPCA can be used for various remote sensing applications. The main dimensionality reduction function is embodied in feature extraction. As early as 2004, Munyati has used PCA to monitor changes in the Kafue wetland in Zambia. The study ran PCA on 12-band imagery to produce colour composite images that highlighted changes in the wetland system.A study by Estornell et al. in 2013 also demonstrated the feasibility of PCA being able to extract land information. After applying PCA to LandSat imagery of the province of Valencia, the study demonstrated that the main components are still capable of detecting forested areas affected by forest fires. However, PCA also exists with certain problems, and when the FEATURES are linearly indistinguishable, often the performance of PCA is unsatisfactory.\nSimilarly, in the field of remote sensing image fusion, Zhang et al. (2016) proposed a method called Sparse Representation and PCA (SPCA), which can effectively simulate intra- and inter-channel correlations to achieve image fusion. Nowadays, there are also methods such as kernel-PCA that first map the data points into a high-dimensional space so as to perform the division of the maximum variance.\n Source: Enes Zvornicanin, 2024"
  },
  {
    "objectID": "Week03.html#reflecion",
    "href": "Week03.html#reflecion",
    "title": "Week 03 Remote sensing data",
    "section": "3. Reflecion",
    "text": "3. Reflecion\nFor the reflection part I chose to focus on enhancements. The concepts of calibration are actually all relatively clear, and very often some calibration (e.g. atmospheric correction) is done at the time of sensor collection. But PCA in image fusion really caught my eye, and I can see that I will have the opportunity to put it into practice in a future Practical. Therefore, I would like to focus more on this concept that I find useful and will use in the future.\nPCA is also from undergraduate exposure to postgraduate studies. For me, it was a poorly understood concept. But as I continued to learn about it, I slowly began to understand the concept better. At its core, it’s like when you’re taking a group photo and you can get the most people in the picture from that one angle. To get more people in the picture, the angle has to ensure that everyone is standing as sparsely as possible, i.e. the variance has to be high. This is the core of PCA, maximising information retention.\nAlthough PCA itself has some limitations, such as linearly indistinguishable data that can have a big impact on performance. But the good thing is that it works well in the vast majority of cases. I remember the first week of Practical with that 1.16G dataset. If I had to deal with a file of that size for every operation, it would be a challenge to my time and computing resources. But if I can use PCA to retain valid information, perhaps this will allow me to do my job of analysing the data more efficiently. Starting to look forward to READING WEEK to come back to practical."
  },
  {
    "objectID": "Week04.html#summary",
    "href": "Week04.html#summary",
    "title": "Week 04 SAR applications & Policies",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W4 Summary\n\n\n\n1.1. Synthetic Aperture Radar (SAR)\n\n1.1.1. Background info\nSynthetic Aperture Radar, or SAR for short, is a type of active sensor. Unlike passive remote sensing, active sensors are not affected by weather, cloud cover, or light. The time delay and phase difference of the reflected signals are recorded by actively transmitting microwave pulses to generate surface images.\nIn addition to being unaffected by weather and light, SAR is also highly penetrating and can penetrate vegetation (mentioned in the Wavelength section).\n\n\n1.1.2. Polarization\nThe concept of polarization was mentioned in Week1. However, in Week4, different types of polarization are introduced, such as horizontal polarization (HH), vertical polarization (VV), and cross-polarization (HV).\n Source: Dabboor and Brisco, 2018\nDifferent types of features have different reflection and scattering characteristics of radar signals. For example, bare rock or ground is more sensitive to vertical polarization and scatters in the same direction as the vertical polarization, making it easier to capture this type of signal; leaves and branches in vegetation respond differently to radar waves, resulting in signals that vary in different polarization modes, and are therefore more sensitive to cross-polarization; in vertical structures, such as trees and buildings, the radar wave is first in contact with the ground, and then reflected again to the receiver, and horizontal polarization is the most sensitive for such cases.\n\n\n1.1.3. Wavelength\nDifferent wavelengths have different usage scenarios, for example, radar systems with shorter wavelengths have higher resolution and are able to display features more clearly. However, the penetration ability for atmosphere and vegetation is poor (Ottinger and Kuenzer, 2020), and it is not easy to obtain information on the surface obscured by vegetation.\n Source: Ottinger and Kuenzer, 2020\nRadar systems of different wavelengths respond differently to polarised scattering, e.g. more scattering is produced by features on rough surfaces (The SAR Handbook).\n Source:Ottinger and Kuenzer, 2020\n\n\n\n1.2. Applications of SAR\n\nPhase and Amplitude Data: The SAR actively transmitting wave records information by scattering from the target, which is recorded in two main ways: phase and amplitude. Phase is used to determine the distance from the sensor to the target, while amplitude indicates the amount of transmitted signal returned to the sensor. Amplitude and phase measurements provide information about the roughness, geometry, and humidity of the target.\n\nSAR has many applications, such as monitoring forests and illegal logging (Hansen et al., 2010), drought monitoring, forest fire monitoring (Morante-Carballo et al., 2022), and * temperature and heat island effect research in P48-P50 (Li et al., 2022) (which also will be mentioned in Week5).\n\nInterferometric Synthetic Aperture Radar (InSAR): SAR-based extensions use the phase difference between two or more radar images to derive elevation information about the surface. It is mainly used in topographic surveys and surface deformation monitoring (e.g., earthquakes) Data processing for InSAR involves interferometric image generation, creation of elevation models, etc.\n\n\n\n1.3. Polocies\n\n\n\n\nPolicies\n\n\n\n\nGlobal\nNew Urban Agenda, Sustainable Development Goals\n\n\nMetropolitan\nPolicy SI 12 Flood risk management, OneNYC 2025\n\n\nLocal\nCape Town Municipal Spatial Development Framework\n\n\n\nPolicies at the global level consist mainly of the New Urban Agenda and the SDGs, both of which I believe emphasise the core concept of sustainability. The New Urban Agenda stresses how to build an inclusive, safe, resilient and sustainable city, as do the Sustainable Development Goals. In other words, ensuring that urban development meets human needs without negatively impacting the environment and society. But the literature doesn’t seem to be able to integrate policies with human needs very well, and the question of Page 53 is well worth thinking about.\nPolicies, whether metropolitan or local, are considered to be detailed to a particular city. Global policies are used more to monitor and understand the changing conditions of the earth as a whole, while local policies are more local and specific, more directly affecting the lives of citizens and the sustainable development of cities. Policies in these cities are mainly concerned with disaster monitoring, land use, environmental monitoring, and post-disaster reconstruction, and remote sensing can provide data and information in this part. For example, remote sensing data combined with models and algorithms can be used to predict and simulate environmental phenomena, thus helping policy makers to visualise the impact of different policies and providing them with decision support."
  },
  {
    "objectID": "Week04.html#application",
    "href": "Week04.html#application",
    "title": "Week 04 SAR applications & Policies",
    "section": "2. Application",
    "text": "2. Application\n\n2.1. Introduction\nI have chosen Mumbai as the city for my research, which is located in the western Indian state of Maharashtra. As one of the largest cities in India, Mumbai is the economic, financial and commercial centre of India and one of the busiest ports in the country. Since the Air Quality Index (AQI) of Mumbai has been maintained at unhealthy levels for a long time, while there are many news reports about Mumbai’s poor air quality and the failure of construction sites to be regulated, I would like to study India whether the Mumbai Municipal Corporation has enacted some policies to control air pollution. In addition, can remote sensing data help government workers to formulate better policies to protect the environment in which people live.\n\n\n2.2. Background\nAccording to the historical data of Air Quality Index of AQI.IN, Mumbai is rarely able to achieve Good air quality in the year 2023, while London is almost the exact opposite.\n Source:AQI.IN  Source:AQI.IN\nAccording to The Time of India reports in 2022 and 2023, air pollution continues to be a serious problem. And according to reports, PM2.5 in the homes of people living near construction sites exceeds 300µg/m3, which is even more than three times the official figure (69.5µg/m3), compared to the UN’s healthy range of &lt;10µg/m3.According to CARB’s information, PM2.5 can have adverse health effects, including premature death, bronchitis, asthma attacks, respiratory symptoms, and debilitating lung function development in children are associated.\n\n\n2.3. Policies\n\n2.3.1. Global\nTo allow air pollution to become so severe is contrary to commitments (b), (c), and (d) of Article 4 of The United Nations Framework Convention on Climate Change (UNFCCC); 65, 75, and 79 of The New Urban Agenda; “Goal 3, 11, 13 of The Sustainable Development Goals.\n Source:UNFCCC, The New Urban Agenda, SDGs\nOverall, Mumbai’s current unsustainable building patterns in the city, greenhouse gas emissions that far exceed standards, and particulate emissions that are harmful to people’s health are all contrary to global policies. Malathy Iyer’s (2023) research suggests that Mumbai’s air pollution stems not only from unsustainable construction modes, but also from construction-induced reductions in road surfaces, combined with Mumbai’s over 120,000 car ownership. Vehicles need to spend more time on the road, which leads to an increase in NOx and pollutant emissions. In addition, NOx generates oxygen radicals in a photochemical reaction, which combine with oxygen molecules to generate abrasive and corrosive ozone, further harming people’s health.\n\n\n2.3.2. National and Local\n  Source: Mumbai Climate Action Plan 2022\n Source: Government of India\nPolicies at the Metropolitan and Local levels are more focused on practical implementation due to the contribution of total suspended particulate matter (PM) emissions from construction activities and pollution from traffic exhaust. The emphasis is on safeguarding the health of citizens through sustainable transport and air quality management. Mumbai Climate Action Plan 2022 states that reliance on ground-based monitoring stations limits the scope and breadth of analyses, and thus fails to identify past patterns and future trends. Remotely sensed data can go some way towards filling critical data gaps.\n\n\n\n2.4. Appling Remote Sense Data and Methods to Achieve Policy Goals\nAccording to Lim et al. (2009), their proposed algorithm is able to calculate PM10 concentrations using remotely sensed data and allows for better visualisation of mapping PM10 concentrations. The source of the data is Landsat TM 5 and according to their proposed algorithm it was found that there is a linear relationship between Atmosphere Reflectance and the change in PM10:\n\\[\nR_{atm}(\\lambda) = \\frac{S}{4\\mu_s\\mu_v} [\\sigma_aP_a(\\Theta, \\lambda) + \\sigma_rP_r(\\Theta, \\lambda)]\n\\]\nIn addition, Scheibenreif, Mommert, and Borth (2021) used deep learning models to predict ambient air pollution. Their use of optical satellite imagery and remote sensing data (Sentinel-2 Level-2A data) avoids the local and temporal limitations of traditional data. And they are able to zoom in on air pollution estimates at high spatial resolution to arbitrary locations, identify major sources of air pollution and greenhouse gases, and monitor them over time.\n\n\n2.5. Reflection of Applications\nBoth studies contribute to the fulfilment of policy goals or the solution of problems, and the application of remotely sensed data allows researchers to have a more dynamic and complete view of climate and air quality. In the former, mathematical methods and regression algorithms were used to demonstrate that PM10 concentrations can be calculated from atmospheric reflectance, which, however, is related to top-of-atmosphere reflectance and surface reflectance. Although the theoretical additive and subtractive relationships are stated, the errors in the tests are not taken into account. The latter is the prediction of pollutant concentration by machine learning algorithms, unfortunately due to the limitation of the dataset they were only able to obtain an R2 of 0.25. Although the pre-training provided by the migration learning improved the model’s performance to a certain extent (R2=0.45), the MAE was still 6.62±0.17 μg/m3, which means that the model still did not have a very good performance. However, as the field of deep learning develops, more powerful models can hopefully provide more accurate pollution predictions to help managers create sustainable cities and improve the health of citizens."
  },
  {
    "objectID": "Week04.html#reflection",
    "href": "Week04.html#reflection",
    "title": "Week 04 SAR applications & Policies",
    "section": "3. Reflection",
    "text": "3. Reflection\nThis week’s class is relatively theoretical, and after introduction of first week, the concepts and workings related to SAR are well understood. But InSAR is still a challenging concept and particular application of SAR. But maybe not every application needs to be very clear about every detail, mastering the overall process as well as the overall functionality is enough for me to use and search for related detailed applications. The Case provided by P48-P50 is also very helpful, combining the classic framework of the thesis, and showing how to use remote sensing as a data source and technology in solving practical problems in the section of Data and Methodology.In fact, remote sensing data is not a very complicated and difficult concept, and this case largely reduced my unfamiliarity with the concept of remote sensing.\nRegarding the part about policies and remote sensing, I had a question when I was listening to the lecture: what is the relationship between these policies and remote sensing? After doing some searching, I found that this relationship can be in two parts. The first part is “pre-policy”, where remote sensing data can be used in conjunction with models and algorithms to visualise different outcomes for planners before a policy is issued, helping them to decide how the policy should be made. The other part is “post-policy”, where policies have been issued in response to specific phenomena (e.g., heat, floods, droughts, etc.) due to the presence of those phenomena. Remote sensing can be used as a technology to help governments and organisations to better understand and respond to environmental challenges. Supporting them at the data level to make the best use of resources.\nOne of the great things about this lecture is that it made me realise that not every detail is so important, and that it is much clearer to have a general framework and process in place first. So many of the policies in the second half of the class were still a little vague for me, despite being broken down into Global, Metropolitan, and Local. However, when I really studied them in detail, and read the policies in detail, I realised that I could actually understand them in a different way: global policies provide the general framework, and urban policies are implemented in different aspects of urban management with remote sensing data in order to fulfil the vision of the global policies.\n\n\n\nConnection within Policies in my opinion"
  },
  {
    "objectID": "Week06.html#summary",
    "href": "Week06.html#summary",
    "title": "Week 06 Introduction to GEE",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W6 Summary\n\n\nThis week’s main focus is to have a brief introduction to GEE. Firstly the language used by GEE is JavaScript, a programming language that is a bit like Python and R. It’s not the same as R, but it’s not the same as R or Python. But unlike R or Python which run either on a rented server or on a local machine, GEE’s code runs partly on the browser and partly on the server side. It’s worth noting that variables on the server will start with ee. And to avoid wasting resources by loading the image multiple times during the loop, GEE can create a function and save it as an object (variable) and then apply it to everything on the server, similar to the map() function in R.\nIn GEE, a bunch of raster data is stored as an image collection, and the class provided a small example of using an image collection: selecting time intervals and regions to avoid error messages caused by large amounts of data. The following is a preliminary application of Practical: representing ROI on a map by selecting time intervals and geographic regions.  With only four or five lines of code, it seems to run at an unparalleled speed compared to R. Another commonly used Object is Features, where the ROI just defined a spatial area, but no attribute information. Therefore, Features can be interpreted as Geometry (points, lines, surfaces) with object information, such as name, type, population, etc.\nGEE can also do data preprocessing and apply some algorithms that we do in R or Python. First of all, Image Reducing, in my understanding, is to turn multiple images into a single image through certain calculations, to avoid instability caused by clouds or seasonal changes. It can also be achieved by region or neighbourhood, the process is very similar to the convolutional neural network convolutional calculation, there is an irregularly shaped window sliding over the image, and some calculations are made on the pixels (such as the median). This can be implemented via the reduceRegions() or reduceNeighbourhood() functions.\nSecondly, GEE can also merge data as we do in R (left join instead of merge). In GEE, there are various types of links, such as Inner Join which overlaps by matching, Spatial Join which connects according to spatial location and attributes, etc. In Practical, I was most shocked by the power of GEE in calculating the data in one go. In Practical, I was most shocked by GEE’s powerful computational capability, which can still run at a relatively fast speed even when calculating PCA with 21 layers at once. \nThirdly, GEE can perform some linear regression. Usually it is the change of a certain variable over time, such as rainfall or temperature. Then the direction and degree of change is represented by the slope, which is visualised by different colours and shades. In addition, when I browsed the official GEE documentation, I found that the objects to apply linear regression can be ee.ImageCollection, ee.Image, ee.FeatureCollection, and ee.List objects."
  },
  {
    "objectID": "Week06.html#application",
    "href": "Week06.html#application",
    "title": "Week 06 Introduction to GEE",
    "section": "2. Application",
    "text": "2. Application\n Source: Google Earth Engine\nThe official website of Google Earth Engine provides a number of applications, including but not limited to population density distribution, forest change visualisation, land use, climate and weather monitoring applications. These applications are all open source, and you can find the corresponding data and code on the official website.\nGoogle provides us with powerful cpu and gpu can batch parallel processing of computational data, but in the process of using it I found that GEE still has some limitations. Firstly, it is expensive to learn, although it provides a seemingly clear page for user interaction, it cannot run a cell individually like Jupyter notebook or R. When I test the changes brought by a certain line of code, I need to wait for the previous code to finish running, which greatly increases the cost of debugging the code. Although GEE can process large images quickly and efficiently based on pyramids at different scales, this is based on the premise that the network is stable. If it is not working properly or is limited in places with bad signal, this is a relatively low degree of freedom for GEE compared to locally running R or Python. The final thing is that when I checked the official documentation, there was no similar tutorial for multiple linear regression. And shockingly, Google Earth Engine can’t actually store and process InSAR data. According to Sentinel-1 algorithms, this is because InSAR data is incompatible with the concept of infrastructure tiling. This makes it impossible to analyse the 3D displacement information of the surface in GEE and we are forced to use less compatible software like SNAP."
  },
  {
    "objectID": "Week06.html#reflection",
    "href": "Week06.html#reflection",
    "title": "Week 06 Introduction to GEE",
    "section": "3. Reflection",
    "text": "3. Reflection\nI am very excited to learn about GEE, despite the various problems in using it, GEE’s powerful computational power and responsiveness still allow me to process remote sensing data efficiently for free. Although it took me a while to get used to writing JavaScript and the GEE UI, I would like to say that the official documentation provided by GEE is a great reference. Both the Guide and the Data Catalog were particularly helpful in getting me started with GEE. In addition, my previous learning of R and Python helped me to get used to using JavaScript more quickly, as the underlying logic of programming is similar, with some syntactic differences.\nThis week I’ve just been following practical and trying to play around with GEE for the time being, everything is new to me, for example the named folders are not allowed to be renamed, and the plots seem to be in a fixed format and not restrained like ggplot or seaborn. I might be able to look into those awesome applications more in the future."
  },
  {
    "objectID": "Week08.html#summary",
    "href": "Week08.html#summary",
    "title": "Week 08 Classification II",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W8 Summary\n\n\n\n1.1 Object-based Image Analysis\nLast week I was lamenting how computers are supposed to recognise categories like the human eye. It turns out it’s through data computation. But in the midst of this week’s study came an algorithm that is very similar to human visual perception: object-based image analysis (OBIA). “Think objects, not pixels” is how I would summarise this approach. The main things involved in this method are segmentation and classification.\n Pagosa Springs, Colorado, USA: Semi-automated object based classification of 1m 4-band NAIP. Classes include trees, lower vegetation, impervious type surfaces and hydrological features.\nSource:Landinfo\nFirstly, there is segmentation, where obia takes a small pixel image and splits it into vector objects. Simply understood, this is the process of grouping similar pixels into objects, much like the human eye does. Once you have these segmented objects, you can use its spectral, geometric, and spatial properties to divide the land cover. I view each segmented object as a Feature, a gemtry that contains additional information. then comes the classification, obia will use the shape, size and spectral properties of the object to classify each object. This is an iterative process, according to the motion picture provided in class. That is, by iteratively adjusting the segmentation parameters (e.g., segmentation scale, shape, and compactness parameters), the most suitable segmentation result for a particular application can be found.\nThe following results based on OBIA and using the CART method to assign land cover categories show the land cover categories for the Tanzanian capital city (urban is red, bare-earth is grey, grass is light yellow and forest is green):\n\n\n\nPractical Result\n\n\nThe results do seem to be missing some detail, perhaps due to the presence of superpixels. The next method introduced is able to show the information inside the pixel more clearly, which is Sub-pixel Analysis.\n\n\n1.2 Sub-pixel Analysis\nEven within a single pixel, different substances on the ground (e.g. different types of vegetation, soil or water) can be mixed. However, in conventional analyses the reflectance value of a pixel is a weighted average of the material reflectance of the material within the pixel. In most cases, this leads to a loss of information. Therefore, sub-pixel analysis aims to distinguish and quantify the proportion of each component in these mixed pixels, thus recovering some of the lost spatial information. The following is the result obtained by applying sub-pixel analysis in practical:\n\n\n\nPractical Result\n\n\nIt can be clearly seen that there are more details in the classification than OBIA, which seems to make the results more realistic and reliable. But how to quantitatively evaluate the results is also a problem worth thinking about, so the next part is the Accuracy assessment.\n\n\n1.3 Accuracy assessment\nThe main centrepiece of the assessment of the accuracy of classification problems is the Confusion Matrix. The Confusion Matrix, which is used to show the relationship between the actual categories and the model’s predicted categories, provides an intuitive way to understand the model’s performance on each category, including its accuracy, false alarm rate, miss rate, and other key metrics.\n Source: Draelos, 2019\nIn a binary classification problem, the confusion matrix consists of four components: - True Positives (TP): the number of classes that the model correctly predicts as positive. - False Positives (FP): the number of classes that the model incorrectly predicts as positive (actually negative). - True Negatives (TN): the number of classes that the model correctly predicts as negative. - False Negatives (FN): the number of negative classes that the model incorrectly predicts (actually positive classes).\nThe confusion matrix allows for the calculation of a variety of assessment metrics:\nAccuracy: the proportion of all correctly predicted observations to the total number of observations. The formula is\n\\[\n\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n\\]\nPrecision: the accuracy of the positive class prediction, i.e., the proportion of true examples that are predicted to be positive. The formula is\n\\[\n\\text{Precision} = \\frac{TP}{TP + FP}\n\\]\nRecall, also known as the true instance rate: the proportion of positive class observations captured by the model. The formula is\n\\[\n\\text{Recall} = \\frac{TP}{TP + FN}\n\\]\nF1 Score: the reconciled average of precision and recall, used to measure the balance between model accuracy and recall. The formula is\n\\[\n\\text{F1 Score} = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n\\]"
  },
  {
    "objectID": "Week08.html#application",
    "href": "Week08.html#application",
    "title": "Week 08 Classification II",
    "section": "2. Application",
    "text": "2. Application\nThe first is that there is an open source machine learning website on EO training.\nMatarira et al. show that the convenience of the simple non-iterative clustering (SNIC) algorithm in Google Earth Engine (GEE) offers Geographic Object-Based Image Analysis (GEOBIA) the potential to map the spatial morphology of deprived neighbourhoods in the complex built environment of Durban. They demonstrated the effectiveness of OBIA in GEE mapping applications by integrating Sentinel-1, Sentinel-2 and PlanetScope satellite data to enable object-based mapping of informal settlements in GEE. But OBIA’s constant search for segmentation classification steps can be very time-consuming when encountering large-scale or high-resolution datasets. And I think one of the major factors for the success of this research is that the data provided by PlanetScope is of high enough resolution so that OBIA can segment and classify the objects well.\nVos et al. completed the development of CoastSat on the GEE platform, which utilises sub-pixel resolution boundary segmentation for shoreline detection and possesses high accuracy. However, it is rather unfortunate not to see comparisons between different models. For example, would there be a difference in performance between a linear spectral unmixing model and a linear combination of reflectance. And the article doesn’t provide an in-depth explanation of how the algorithm determines the proportion of surface types within a pixel, perhaps also because the models themselves are less interpretable. But nonetheless, the results of the algorithm runs can provide accurate results and INSIGHTS for policy makers, or maybe that’s the beauty of sub-pixel analysis."
  },
  {
    "objectID": "Week08.html#reflection",
    "href": "Week08.html#reflection",
    "title": "Week 08 Classification II",
    "section": "3. Reflection",
    "text": "3. Reflection\nParente et al. (2019) evaluated the performance of machine learning approaches, including deep learning algorithms implemented using TensorFlow, for mapping pasturelands in Central Brazil. Wang et al. (2020) proposed a method that combines GEE with a multiscale convolutional neural network (MSCNN) for urban water extraction from Landsat images. Balaniuk et al. (2020) explored the use of deep learning methods to detect surface mines and mining tailings dams in Brazil using multispectral Sentinel-2 satellite imagery processed in the GEE platform.\nMore and more research is trying to combine GEE and deep learning, and this trend is really in line with the main theme of the AI era. Although deep learning frameworks or models are still like a black box, it is not easy to explain the principles behind the theories. But from a practical point of view, it can provide more advanced recognition or more accurate classification, offering potential solutions to more complex problems. Like Sub-pixels analysis, it may not be necessary to specify exactly how to infer the proportion of land types within a pixel, just learn to use it and analyse the results. After this week’s study and research, I’m even more convinced about last week’s resource integration. Despite the advantages that GEE already has, Jean has some disadvantages that need to be supplemented by other resources. Hopefully in the future I can empower GEE with deep learning as well."
  },
  {
    "objectID": "Week09.html#summary",
    "href": "Week09.html#summary",
    "title": "Week 09 SAR in GEE",
    "section": "1. Summary",
    "text": "1. Summary\nSince the content of the class is highly similar to that of Week4, I won’t summarise it too much. There will be some new content including t-tests that will be brought through in conjunction with Practical in the Application section. If you’re reading this and want to learn about SAR, click this link."
  },
  {
    "objectID": "Week09.html#application",
    "href": "Week09.html#application",
    "title": "Week 09 SAR in GEE",
    "section": "2. Application",
    "text": "2. Application\nThe Practical this time was to assess the blast damage, and according to information provided by NASA, Sentinel-1 synthetic aperture radar (SAR) imagery is well suited for generating estimated damage maps. I think it also has to do with what I learnt about SAR when I studied it earlier: SAR is an active sensor that emits microwave pulses that can penetrate clouds and vegetation and is unaffected by light, so it can detect changes that visible light images cannot show. And when the terrain changes, the amplitude and phase of the radar waves change as a result, which is an important reason why SAR can be used to view changes in the ground. Although NASA does not provide the code and instructions for evaluating explosions, this Practical is able to reproduce the change monitoring algorithm. This is the result I completed with reference to Practical:\n\n\n\n\n\n\n2.1 Change detection\nDetecting changes between images can be initially recognised by a simple phase reduction method of comparing images before and after an event, but this method may make it difficult to accurately distinguish the cause of the change. Considering the continuous changes in the urban environment, a more effective method is to compare a large number of images to confirm the consistency of the changes. In order to more accurately assess changes, in addition to using average pixel values, standard deviations are calculated to account for fluctuations in pixel values. By applying these statistics to a pixel t-test, the significance of the change can be quantitatively determined. If the t-value is greater than 2, the change is considered significant; if it is less than 2, the change is not significant, thus accurately identifying image changes caused by specific events. Formulas are:\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\n\\[\ns = \\sqrt{\\frac{1}{n - 1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2}\n\\]\n\\[\nt = \\frac{\\bar{x}_1 - \\bar{x}_2}{\\sqrt{\\frac{s_1^2}{n_1} + \\frac{s_2^2}{n_2}}}\n\\]\nHowever, since the image contains both ascending and descending orbits, the difference in angle will bring some noise. A function called “filter_s1” is defined in Practical to filter the orbits to reduce the effect of noise. Then the two tracks are used separately and the results are merged into one image. Afterwards, the ROI (Region of Interest) is shown on the map.\n\n\n\nRegion of Interest\n\n\nThe visualisation parameters correspond to the statistical significance of the change in pixel values. Using the Viridis palette from purple to yellow, dark purple pixels indicate no significant change and yellow pixels indicate a significant change with 95% confidence. The brighter the yellow colour of the pixel, the more significant the change. But however some other changes in the harbour could be misjudged, such as ships coming in and out, cranes moving, and containers being loaded and unloaded are all recorded in the change detection algorithm. But the good thing is that Practical mentions among other things that the scale and number of changes in 2018 are much less than the images from 2020, suggesting that the algorithm is detecting the changes brought about by the explosion.\n\n\n2.2 Validation\nThe algorithm appears to be effective. But how do we know that it correctly predicts the extent of damage and is not over- or under-estimating it? The answer is that using the building footprint data and the t-test image we just generated, we can create an estimate of the number of damaged buildings based on the model. First, we generate a thresholded image, setting pixels with values greater than 0 to 1 and others to 0. We can then use this mask to reduce the building footprint to a single value for each building that is the average of the t-test images within the footprint. If the average value is greater than 0, the building is damaged. If it is less than 0, the building is not damaged.\n\nBlue = no damage\nGreen = low damage\nYellow/Orange = medium damage\nRed = high levels of damage\n\n\n\n\nValidation"
  },
  {
    "objectID": "Week09.html#reflection",
    "href": "Week09.html#reflection",
    "title": "Week 09 SAR in GEE",
    "section": "3. Reflection",
    "text": "3. Reflection\nSimilarly, the t-test I learnt this week was like applying random forests in the previous two weeks. The process of going from theory to practice is something I really enjoy, not only does it give me an additional perspective to refine my perception, but it also strengthens the application of my knowledge even further. Moreover, the topic of this practical is explosion assessment, which is also different from the previous land use classification. It showed me more possibilities of remote sensing data and algorithms. Maybe in the future we can also use remote sensing data to monitor traffic flow or illegal buildings in the city.\nFinally, in the last part of my learning diary, I would like to summarise some of the changes that this course has brought me. First of all, it’s the divergent thinking, which can’t be brought to a lot of content in the limited classroom time. But we can find the materials we need through self-study on the Internet, which is rich in resources. Then there’s the “less is more” mindset. I wrote too much before reading week, so I’m taking a more in-depth view in the last four weeks. I hope to be able to be concise enough while still being clear about the information I want to share. Lastly, the integration of resources and the use of tools, the biggest difference between humans and machines is that humans have access to tools, GPT or search engines can help my learning to a certain extent, and I hope to use them more in the future."
  },
  {
    "objectID": "Week07.html#summary",
    "href": "Week07.html#summary",
    "title": "Week 07 Classification I",
    "section": "1. Summary",
    "text": "1. Summary\n\n\n\nMindmap of W7 Summary\n\n\n\n1.1 Classification and regression trees (CART)\n Source:Datacamp\nDecision trees must be the first model to be proposed when referring to classification algorithms, whether in CASA0023 or CASA0006 (Data Science for Spatial Systems).CART is a predictive model that can be used for both classification and regression.\nThe basic steps of the CART algorithm are as follows:\n\nFirst split the dataset into if two subsets based on an evaluation criterion (regression tree: MSE; classification tree: Gini impurity). Since the topic of this chapter is classification, the main summary is the classification purpose of CART. Gini impurity in my understanding is whether the data in the split sub-dataset are similar. If the data in the sub-datasets are all similar, or all of them are of one class, then such a split is successful.\nThen repeat the splitting process until the stopping conditions are met (e.g., the maximum depth is reached, the evaluation criteria reaches a threshold).\nFinally, to avoid overfitting, the complexity of the tree needs to be reduced by pruning. Usually there is a cost complexity parameter to control. In practice, this can be done through various packages, such as the ccp_alpha parameter in scikit-learn.\n\nBelow is an example of CART classification of land use in Milan using LANDSAT data: \n\n\n1.2 Overfitting\nOverfitting is a common problem in machine learning, referring to models that perform well on training data but poorly on new, unseen data. Overfitting models learn the noise and errors in the training data, as well as overly complex patterns that don’t apply to new data.\nIn CART we can prevent overfitting by pruning, but in general we can choose to split the dataset into a training set and a test set. The test set provides a perspective on how the model performs on new data, as well as a way to determine if the model is starting to overfit the data in the training set. Additionally, my own previous use of machine learning algorithms has often included cross-validation to ensure that the model performs well on different subsets of data to assess the model’s ability to generalise. Or the risk of overfitting is reduced by some integration algorithm, such as random forests, which will be covered next.\n\n\n1.3 Random Forest\n Source:JINSOL KIM\nA random forest is, as its name suggests, an integrated model consisting of multiple CART. Each tree is trained on a different random subset of the dataset. In the classification task, each tree in the random forest produces a classification result, and then a voting mechanism is used to determine the final category.\nSince the trees in a random forest are built independently, the overall model is less prone to overfitting, especially if there are enough trees and enough depth. And compared to XGBoost’s constant iteration of single-class trees, the parallel computation of random forests can save a lot of time.\nBelow is an example of CART classification of land use in Milan using LANDSAT data: \nI’ve tried to zoom in on the classification results as much as possible, and was able to make it relatively obvious (I guess) that Random Forest gives better results than CART, with a much finer level of classification.\n\n\n1.4 Maximum Likelihood\nThe maximum likelihood method used to be a nightmare for me, as learning this algorithm as an undergraduate required hand-calculating the mathematical formulas behind it. Even though it has reappeared in my graduate studies, I am no longer afraid of it. The important part of this algorithm is that it requires the assumption that the probability distribution of each category over each feature is known, usually we assume a Gaussian distribution is satisfied. Then, the training data is used to estimate the statistical parameters for each category, i.e., the mean and covariance matrices. Next, for each pixel to be classified, the probability that it belongs to each category is calculated based on its features. Finally, the probabilities of each category are compared and the pixel is assigned to the category with the largest a posteriori probability. But I personally think there is a very fundamental problem with this algorithm, which is that this precondition that the a priori probability is known is too idealistic. Often, this condition cannot be met, and therefore I have rarely seen this algorithm in practice.\n\n\n1.5 Support Vector Machine (SVM)\nThe core of a support vector machine is to find an optimal hyperplane that maximises the boundary between two types of data in a binary classification task. That is, maximising the distance from the training data points to this decision plane. When dealing with non-linearly divisible data, SVMs map the data to a higher dimensional space via a kernel function, as shown below. (A Gaussian kernel function is usually chosen when dimensioning; when the data is linearly differentiable, the kernel function is usually chosen as a linear kernel function.)\n Source:Rohith Gandhi\nTo avoid overfitting, SVMs introduce a concept of Soft Margin, which was mentioned in class as allowing misclassification to occur. But it is not explained in depth too much, so I would like to expand some information moderately in this small piece. Soft Margin classification is achieved by introducing so-called slack variables (ξ), one for each point. Since a data point has a slack variable of 0 if it is on the correct side and its distance from the hyperplane exceeds the interval, the SVM needs to not only maximise the interval but also minimise the slack variables. So how can this be done? This can be done by adding a regularisation term to the objective function, which is usually the sum of the values of the slack variables, multiplied by a regularisation parameter C. This parameter C is a hyperparameter, specified by the user. When C is large, the model is prone to overfitting; when C is small, the model is prone to underfitting. Therefore, the value of C is important in the process of tuning the parameter."
  },
  {
    "objectID": "Week07.html#application",
    "href": "Week07.html#application",
    "title": "Week 07 Classification I",
    "section": "2. Application",
    "text": "2. Application\nPhan, Kuch and Lehnert (2020) used Landsat 8 surface reflectance (L8sr) data and a random forest algorithm on GEE to study land cover in the Mongolian region. The random forest algorithm provided a relatively high level of accuracy, with an overall accuracy of over 84.31 per cent.\n Source:Phan, Kuch and Lehnert, 2020\nShelestov et al. (2017) explored the efficiency of GEE for classifying multi-temporal satellite imagery for crop mapping, finding that while GEE provides good performance in terms of access and pre-processing, neural network-based approaches outperformed classifiers like random forest available in GEE.\n Source:Shelestov et al., 2017\nWhen looking for applications of classification algorithms on GEE, I found that the vast majority of the research used more basic algorithms such as Random Forest or Gradient Tree Boosting. As Shelestov et al. suggest, neural networks are better at dealing with more complex problems, but it seems that GEE doesn’t directly provide built-in support for these types of models.However, GEE does seem to have an API option in the UI, so it may be possible to train models in external environments (deep learning frameworks such as TensorFlow, PyTorch, etc.) and then import them into GEE for use. This may be a future direction to improve the accuracy and robustness of remote sensing classification."
  },
  {
    "objectID": "Week07.html#reflection",
    "href": "Week07.html#reflection",
    "title": "Week 07 Classification I",
    "section": "3. Reflection",
    "text": "3. Reflection\nInitially when I approached this class, I was curious about how I should classify images. Can a machine really tell what category a part of an image belongs to, just like the human eye? What I didn’t expect is that it really can. It turns out that images are also made up of pixels, and algorithms can classify them through computation or judgement. In the process of doing Practical, I found that it is really interesting to find a practical scenario for the theory I learnt before. What I learnt in theory was always that the Random Forest model is poorly interpretable, but in practice when I visualised the classification results, the results provided a lot of insights and were obvious. For me, in the future, I can consolidate the theory with practice.\nIn addition, GEE is a relatively new platform, and I didn’t find too many tutorials during the practical debugging process. Besides referring to the official website of GEE, the content provided by Olie in CASA0025 is also very helpful. Although I didn’t take that course, thanks to Olie for making the website public. From a certain point of view, learning is also a process of resource integration, combining different resources, always find a way to debug. In the future, I can also organise more and collect resources from different parties to help me learn."
  }
]